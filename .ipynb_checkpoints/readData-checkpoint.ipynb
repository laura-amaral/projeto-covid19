{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:33.109952Z",
     "start_time": "2020-04-12T19:43:30.207882Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:33.118930Z",
     "start_time": "2020-04-12T19:43:33.111948Z"
    }
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()+os.sep+'data'\n",
    "file_1 = 'dataset.xlsx'\n",
    "\n",
    "\n",
    "pathNs = os.getcwd()+os.sep+'dataNs'\n",
    "file_2 = 'kaggle_einstein.xlsx'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:34.875134Z",
     "start_time": "2020-04-12T19:43:33.121921Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_excel(os.path.join(path,file_1), encoding= 'latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:34.895081Z",
     "start_time": "2020-04-12T19:43:34.877129Z"
    }
   },
   "outputs": [],
   "source": [
    "explain_df = pd.read_excel( os.path.join(pathNs,file_2), encoding= 'latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:33:58.106798Z",
     "start_time": "2020-04-12T18:33:58.102810Z"
    }
   },
   "source": [
    "- Criar padrões (abstrato mesmo)\n",
    "- Colunas com mesmo tipo (str, float) tirar (nan)\n",
    "- Renomear (colunas com nome maisculo ... exemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:34.906053Z",
     "start_time": "2020-04-12T19:43:34.897076Z"
    }
   },
   "outputs": [],
   "source": [
    "#list of features with medical meaning\n",
    "sel_feat_lst = explain_df[explain_df['clinical_meaning'] == 1]['variable_einstein'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:34.928991Z",
     "start_time": "2020-04-12T19:43:34.908047Z"
    }
   },
   "outputs": [],
   "source": [
    "data_df.rename(columns = {\"Patient addmited to regular ward (1=yes, 0=no)\":\"REGULAR\",\n",
    "                          \"Patient addmited to semi-intensive unit (1=yes, 0=no)\":\"SEMI\",\n",
    "                          \"Patient addmited to intensive care unit (1=yes, 0=no)\":\"UTI\"\n",
    "                         }, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:34.938965Z",
     "start_time": "2020-04-12T19:43:34.929988Z"
    }
   },
   "outputs": [],
   "source": [
    "df = data_df.drop(columns=['Patient ID'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:35.992414Z",
     "start_time": "2020-04-12T19:43:35.984436Z"
    }
   },
   "outputs": [],
   "source": [
    "df['CRITICO'] = ((df['SEMI'] == 1) | (df['UTI'] == 1)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:36.412323Z",
     "start_time": "2020-04-12T19:43:36.392345Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Neutrophils/Lymphocytes ratio'] = df['Neutrophils']/df['Lymphocytes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:54.650773Z",
     "start_time": "2020-04-12T19:43:54.642767Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['Gaso performed '] = ~pd.isna(df[['pH (arterial blood gas analysis)',\n",
    "                                     'HCO3 (arterial blood gas analysis)',\n",
    "                                     'pO2 (arterial blood gas analysis)',\n",
    "                                     'pH (venous blood gas analysis)',\n",
    "                                     'HCO3 (venous blood gas analysis)',\n",
    "                                     'pO2 (venous blood gas analysis)']]).all(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:44:51.977835Z",
     "start_time": "2020-04-12T19:44:51.968827Z"
    }
   },
   "outputs": [],
   "source": [
    "#Curiosidade Laura:\n",
    "df['Gaso performed '] = df['Gaso performed '].apply(lambda x: 0 if x is False else 1)\n",
    "#Tambem funciona:\n",
    "#df['Gaso performed '] = df['Gaso performed '].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:43:36.531972Z",
     "start_time": "2020-04-12T19:43:36.519006Z"
    }
   },
   "outputs": [],
   "source": [
    "df['Urine - Leukocytes'] = df['Urine - Leukocytes'].apply(lambda x: 1 if x is not np.nan else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting features with medical meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:45:50.394431Z",
     "start_time": "2020-04-12T19:45:50.389446Z"
    }
   },
   "outputs": [],
   "source": [
    "sel_feat_lst.extend(['REGULAR','SEMI','UTI','CRITICO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:45:50.661104Z",
     "start_time": "2020-04-12T19:45:50.601265Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df[sel_feat_lst]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore (Report Generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:24:56.230565Z",
     "start_time": "2020-04-12T18:24:50.893763Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-3b618433b98f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mProfileReport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Kaggle Einstein Feature Data'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'style'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'full_width'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'features_df' is not defined"
     ]
    }
   ],
   "source": [
    "profile = ProfileReport(features_df, title='Kaggle Einstein Feature Data', html={'style':{'full_width':True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T18:24:56.237543Z",
     "start_time": "2020-04-12T18:24:49.869Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'profile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-993fc9a3e282>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprofile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"featureReport.html\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'profile' is not defined"
     ]
    }
   ],
   "source": [
    "profile.to_file(output_file=\"featureReport.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Selection (Features and instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:45:54.302012Z",
     "start_time": "2020-04-12T19:45:54.292031Z"
    }
   },
   "outputs": [],
   "source": [
    "lst1 = ['REGULAR', 'SEMI', 'UTI', 'CRITICO',\n",
    "        'Hematocrit','Patient age quantile',\n",
    "        'Neutrophils/Lymphocytes ratio',\n",
    "        'Proteina C reativa mg/dL',\n",
    "        'Platelets', 'Urine - Leukocytes',\n",
    "        'Creatinine', 'Urea', 'Gaso performed ']\n",
    "\n",
    "df1 = df[lst1]\n",
    "\n",
    "lst2 = ['REGULAR', 'SEMI', 'UTI', 'CRITICO',\n",
    "        'Hematocrit','Patient age quantile',            \n",
    "        'Neutrophils/Lymphocytes ratio',         \n",
    "        'Proteina C reativa mg/dL',              \n",
    "        'Platelets', 'Urine - Leukocytes', 'Gaso performed ']\n",
    "\n",
    "df2 = df[lst2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:45:54.878490Z",
     "start_time": "2020-04-12T19:45:54.871510Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = df1[~pd.isna(df1).any(axis=1)]\n",
    "df1.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:45:55.234187Z",
     "start_time": "2020-04-12T19:45:55.226234Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = df2[~pd.isna(df2).any(axis=1)]\n",
    "df2.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:45:58.234125Z",
     "start_time": "2020-04-12T19:45:58.031608Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rodar um RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:20:52.490941Z",
     "start_time": "2020-04-12T20:20:52.484957Z"
    }
   },
   "outputs": [],
   "source": [
    "def prettyReport(dic):\n",
    "    \n",
    "    for i in dic.keys():\n",
    "        print(f'{i}- Treino: {round(dic[i][0],3)} Teste:{round(dic[i][1],3)}')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:22:20.158799Z",
     "start_time": "2020-04-12T20:22:20.137842Z"
    }
   },
   "outputs": [],
   "source": [
    "def makeModelScores(skf, model, X, y):\n",
    "    \n",
    "    scores_dic = {'AUC' : [],\n",
    "                  'PRECISION' : [],\n",
    "                  'RECALL' : [],\n",
    "                  'F1SCORE' : []}\n",
    "    \n",
    "    auc_train = 0\n",
    "    precision_train = 0\n",
    "    recall_train = 0\n",
    "    f1Score_train = 0\n",
    "    \n",
    "    auc_test = 0\n",
    "    precision_test = 0\n",
    "    recall_test = 0\n",
    "    f1Score_test = 0\n",
    "        \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        y_train_hat = model.predict(X_train)\n",
    "\n",
    "        auc_train += roc_auc_score(y_train, y_train_hat)\n",
    "        precision_train += precision_score(y_train, y_train_hat)\n",
    "        recall_train += recall_score(y_train, y_train_hat)\n",
    "        f1Score_train += f1_score(y_train, y_train_hat)\n",
    "        \n",
    "        y_hat = model.predict(X_test)\n",
    "        \n",
    "        auc_test += roc_auc_score(y_test, y_hat)\n",
    "        precision_test += precision_score(y_test, y_hat)\n",
    "        recall_test += recall_score(y_test, y_hat)\n",
    "        f1Score_test += f1_score(y_test, y_hat)\n",
    "\n",
    "    n_fold = skf.get_n_splits(X, y)\n",
    "       \n",
    "    #COLOCA TREINO\n",
    "    scores_dic['AUC'].append(auc_train/n_fold)\n",
    "    scores_dic['PRECISION'].append(precision_train/n_fold)\n",
    "    scores_dic['RECALL'].append(recall_train/n_fold)\n",
    "    scores_dic['F1SCORE'].append(f1Score_train/n_fold)\n",
    "    \n",
    "    #COLOCA TEST\n",
    "    scores_dic['AUC'].append(auc_test/n_fold)\n",
    "    scores_dic['PRECISION'].append(precision_test/n_fold)\n",
    "    scores_dic['RECALL'].append(recall_test/n_fold)\n",
    "    scores_dic['F1SCORE'].append(f1Score_test/n_fold)\n",
    "        \n",
    "    prettyReport(scores_dic)\n",
    "    \n",
    "    return scores_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModelScores(skf, model, X, y):\n",
    "    \n",
    "    scores_dic = {'AUC' : [],\n",
    "                  'PRECISION' : [],\n",
    "                  'RECALL' : [],\n",
    "                  'F1SCORE' : []}\n",
    "    \n",
    "    auc_train = 0\n",
    "    precision_train = 0\n",
    "    recall_train = 0\n",
    "    f1Score_train = 0\n",
    "    \n",
    "    auc_test = 0\n",
    "    precision_test = 0\n",
    "    recall_test = 0\n",
    "    f1Score_test = 0\n",
    "        \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        model.fit(X_train,y_train)\n",
    "\n",
    "        y_train_hat = model.predict(X_train)\n",
    "\n",
    "        auc_train += roc_auc_score(y_train, y_train_hat)\n",
    "        precision_train += precision_score(y_train, y_train_hat)\n",
    "        recall_train += recall_score(y_train, y_train_hat)\n",
    "        f1Score_train += f1_score(y_train, y_train_hat)\n",
    "        \n",
    "        y_hat = model.predict(X_test)\n",
    "        \n",
    "        auc_test += roc_auc_score(y_test, y_hat)\n",
    "        precision_test += precision_score(y_test, y_hat)\n",
    "        recall_test += recall_score(y_test, y_hat)\n",
    "        f1Score_test += f1_score(y_test, y_hat)\n",
    "\n",
    "    n_fold = skf.get_n_splits(X, y)\n",
    "       \n",
    "    #COLOCA TREINO\n",
    "    scores_dic['AUC'].append(auc_train/n_fold)\n",
    "    scores_dic['PRECISION'].append(precision_train/n_fold)\n",
    "    scores_dic['RECALL'].append(recall_train/n_fold)\n",
    "    scores_dic['F1SCORE'].append(f1Score_train/n_fold)\n",
    "    \n",
    "    #COLOCA TEST\n",
    "    scores_dic['AUC'].append(auc_test/n_fold)\n",
    "    scores_dic['PRECISION'].append(precision_test/n_fold)\n",
    "    scores_dic['RECALL'].append(recall_test/n_fold)\n",
    "    scores_dic['F1SCORE'].append(f1Score_test/n_fold)\n",
    "        \n",
    "    prettyReport(scores_dic)\n",
    "    \n",
    "    return scores_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:46:13.403936Z",
     "start_time": "2020-04-12T19:46:13.400940Z"
    }
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Logistico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:46:01.122788Z",
     "start_time": "2020-04-12T19:46:01.117794Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(\n",
    "                solver   = 'sag',\n",
    "                penalty  = 'l2',\n",
    "                max_iter = 10000,\n",
    "                #l1_ratio = 0.9,\n",
    "                n_jobs   = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Com ureia e creatina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T19:46:03.504687Z",
     "start_time": "2020-04-12T19:46:03.496708Z"
    }
   },
   "outputs": [],
   "source": [
    "y1_regular = df1['REGULAR']\n",
    "y1_semi    = df1['SEMI']\n",
    "y1_uti     = df1['UTI']\n",
    "y1_critico = df1['CRITICO']\n",
    "X1 = df1.drop(columns=['REGULAR','SEMI','UTI','CRITICO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T20:13:08.264072Z",
     "start_time": "2020-04-12T20:13:07.553566Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.572 Teste:0.494\n",
      "\n",
      "PRECISION- Treino: 0.833 Teste:0.0\n",
      "\n",
      "RECALL- Treino: 0.147 Teste:0.0\n",
      "\n",
      "F1SCORE- Treino: 0.244 Teste:0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list1_uti = makeModelScores(skf, logreg, X1, y1_uti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.524 Teste:0.503\n",
      "\n",
      "PRECISION- Treino: 0.5 Teste:0.017\n",
      "\n",
      "RECALL- Treino: 0.052 Teste:0.05\n",
      "\n",
      "F1SCORE- Treino: 0.091 Teste:0.025\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "score_list1_semi = makeModelScores(skf, logreg, X1, y1_semi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.546 Teste:0.511\n",
      "\n",
      "PRECISION- Treino: 0.76 Teste:0.133\n",
      "\n",
      "RECALL- Treino: 0.098 Teste:0.047\n",
      "\n",
      "F1SCORE- Treino: 0.165 Teste:0.065\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "score_list1_regular = makeModelScores(skf, logreg, X1, y1_regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crítico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.64 Teste:0.606\n",
      "\n",
      "PRECISION- Treino: 0.701 Teste:0.795\n",
      "\n",
      "RECALL- Treino: 0.299 Teste:0.254\n",
      "\n",
      "F1SCORE- Treino: 0.417 Teste:0.304\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list1_critico = makeModelScores(skf, logreg, X1, y1_critico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem ureia e creatina"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2_regular = df2['REGULAR']\n",
    "y2_semi    = df2['SEMI']\n",
    "y2_uti     = df2['UTI']\n",
    "y2_critico = df2['CRITICO']\n",
    "X2 = df2.drop(columns=['REGULAR','SEMI','UTI','CRITICO'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.54 Teste:0.496\n",
      "\n",
      "PRECISION- Treino: 0.567 Teste:0.0\n",
      "\n",
      "RECALL- Treino: 0.083 Teste:0.0\n",
      "\n",
      "F1SCORE- Treino: 0.143 Teste:0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list2_uti = makeModelScores(skf, logreg, X2, y2_uti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.528 Teste:0.505\n",
      "\n",
      "PRECISION- Treino: 0.72 Teste:0.015\n",
      "\n",
      "RECALL- Treino: 0.057 Teste:0.04\n",
      "\n",
      "F1SCORE- Treino: 0.101 Teste:0.022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list2_semi =  makeModelScores(skf, logreg, X2, y2_semi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGULAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.537 Teste:0.525\n",
      "\n",
      "PRECISION- Treino: 0.633 Teste:0.25\n",
      "\n",
      "RECALL- Treino: 0.08 Teste:0.067\n",
      "\n",
      "F1SCORE- Treino: 0.138 Teste:0.087\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "score_list2_regular =  makeModelScores(skf, logreg, X2, y2_regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRITICO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.592 Teste:0.595\n",
      "\n",
      "PRECISION- Treino: 0.626 Teste:0.567\n",
      "\n",
      "RECALL- Treino: 0.195 Teste:0.222\n",
      "\n",
      "F1SCORE- Treino: 0.294 Teste:0.247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list2_critico =  makeModelScores(skf, logreg, X2, y2_critico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Árvore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(max_depth = 5, \n",
    "                             n_estimators = 500,    \n",
    "                             random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem ureia e creatina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.853 Teste:0.5\n",
      "\n",
      "PRECISION- Treino: 1.0 Teste:0.0\n",
      "\n",
      "RECALL- Treino: 0.707 Teste:0.0\n",
      "\n",
      "F1SCORE- Treino: 0.824 Teste:0.0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "score_list1_uti = makeModelScores(skf, rfc, X1, y1_uti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.891 Teste:0.538\n",
      "\n",
      "PRECISION- Treino: 1.0 Teste:0.05\n",
      "\n",
      "RECALL- Treino: 0.781 Teste:0.1\n",
      "\n",
      "F1SCORE- Treino: 0.876 Teste:0.067\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list1_semi = makeModelScores(skf, rfc, X1, y1_semi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.709 Teste:0.506\n",
      "\n",
      "PRECISION- Treino: 1.0 Teste:0.067\n",
      "\n",
      "RECALL- Treino: 0.418 Teste:0.025\n",
      "\n",
      "F1SCORE- Treino: 0.584 Teste:0.036\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list1_regular = makeModelScores(skf, rfc, X1, y1_regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crítico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.896 Teste:0.624\n",
      "\n",
      "PRECISION- Treino: 1.0 Teste:0.609\n",
      "\n",
      "RECALL- Treino: 0.792 Teste:0.304\n",
      "\n",
      "F1SCORE- Treino: 0.883 Teste:0.322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list1_critico = makeModelScores(skf, rfc, X1, y1_critico)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sem ureia e creatina"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### UTI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 0.983 Teste:0.496\n",
      "\n",
      "PRECISION- Treino: 1.0 Teste:0.0\n",
      "\n",
      "RECALL- Treino: 0.967 Teste:0.0\n",
      "\n",
      "F1SCORE- Treino: 0.983 Teste:0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list2_uti = makeModelScores(skf, rfc, X2, y2_uti)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SEMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 1.0 Teste:0.567\n",
      "\n",
      "PRECISION- Treino: 1.0 Teste:0.057\n",
      "\n",
      "RECALL- Treino: 1.0 Teste:0.16\n",
      "\n",
      "F1SCORE- Treino: 1.0 Teste:0.084\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list2_semi =  makeModelScores(skf, rfc, X2, y2_semi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REGULAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 1.0 Teste:0.525\n",
      "\n",
      "PRECISION- Treino: 1.0 Teste:0.267\n",
      "\n",
      "RECALL- Treino: 1.0 Teste:0.067\n",
      "\n",
      "F1SCORE- Treino: 1.0 Teste:0.093\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list2_regular =  makeModelScores(skf, rfc, X2, y2_regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRITICO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\laura\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC- Treino: 1.0 Teste:0.609\n",
      "\n",
      "PRECISION- Treino: 1.0 Teste:0.6\n",
      "\n",
      "RECALL- Treino: 1.0 Teste:0.247\n",
      "\n",
      "F1SCORE- Treino: 1.0 Teste:0.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score_list2_critico =  makeModelScores(skf, rfc, X2, y2_critico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
